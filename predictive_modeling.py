# -*- coding: utf-8 -*-
"""Predictive_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YmVGXwwarXXrZkOUPMit_cZLSpkGvGsB

<h2>Data Loading</h2>
"""

import kagglehub, os
import pandas as pd
dir = kagglehub.dataset_download('deltasierra452/airline-pax-satisfaction-survey')

print('Data source import complete.')
data = pd.read_csv(os.path.join(dir,os.listdir(dir)[0]))
data

#menggunakan environment kaggle
import pandas as pd
data=pd.read_csv("/kaggle/input/airline-pax-satisfaction-survey/airline_clnd.csv")
data

"""Menghilangkan data-data yang kosong"""

data = data.dropna()
data

"""<h4>Menghapus data duplikat</h4>"""

data = data.drop_duplicates()
data

"""Kolom SR dan ID adalah kolom yang berhubungan dengan identitas pelanggan. Hal ini tidak akan mengganggu proses pemodelan. Sehingga variabel-variabel ini dihapus"""

data=data.drop("SR",axis=1)
data=data.drop("id",axis=1)

"""<h2>Data Exploration</h2>
<h5>Mengelompokkan Data berdasarkan group umur</h5>
"""

age_groups={
    "0-9":data.Age.loc[data.Age<10].shape[0],
    "10-19":data.Age.loc[(data.Age>10)&(data.Age<=20)].shape[0],
    "20-29":data.Age.loc[(data.Age>20)&(data.Age<=30)].shape[0],
    "30-39":data.Age.loc[(data.Age>30)&(data.Age<=40)].shape[0],
    "40-49":data.Age.loc[(data.Age>40)&(data.Age<=50)].shape[0],
    "50-59":data.Age.loc[(data.Age>50)&(data.Age<=60)].shape[0],
    "60-69":data.Age.loc[(data.Age>60)&(data.Age<=70)].shape[0],
    "70-79":data.Age.loc[(data.Age>70)&(data.Age<=80)].shape[0],
    "80-89":data.Age.loc[(data.Age>80)&(data.Age<=90)].shape[0],
}
print(age_groups)

import numpy as np
x = np.array(list(age_groups.values()))
print("Age groups:")
for i in age_groups.keys():
    print(i,"\tPercentage:",age_groups[i]/x.sum()*100,"%")

"""Lebih dari 80% Penumpang berumur 20-60. Kelompok umur difokuskan kepada penumpang berumumur diantara 20 dan 60 tahun"""

#mengidentifikasi penumpang tidak loyal berumumur 20-60
# data=data.loc[data["Customer_Type"]=="disloyal Customer"]
data=data.loc[(data.Age>=20)&(data.Age<=60)]
# data=data.drop("Customer_Type",axis=1)
data=data.dropna()

"""<h3>Mengeksplorasi data target(satisfaction)</h3>"""

count = data["satisfaction"].value_counts()
percent = 100*data["satisfaction"].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title="Satisfaction",rot=0);

"""<h4>Data kepuasan pelanggan menunjukkan bahwa data relatif berimbang. Pengurangan data tidak diperlukan untuk mengurangi bias pada pemodelan</h4>

<h2>Univariate Data analysis</h2>
"""

data.describe().round(2)

import matplotlib.pyplot as plt
data.hist(bins=50,figsize=(20,20))
plt.show()

numerical_features=["Age","Flight_Distance","Departure_Delay_in_Minutes","Arrival_Delay_in_Minutes"]
features=list(data.columns)
for i in numerical_features:
    features.remove(i)
data[features]

"""<h2>Exploratory Data - Numerical Features</h2>"""

data[numerical_features].hist()
plt.show()

"""<h2>Mengidentifikasi outlier pada variabel-variabel numerik</h2>"""

import seaborn as sns
sns.boxplot(data[numerical_features])
plt.xticks(rotation=90)
plt.show()

"""<p>Berdasarkan grafik di atas, terdapat 3 variabel numerik yang memiliki outlier. Variabel tersebut meliputi: Flight_Distance, Departure_Delay_in_Minutes dan Arrival_Delay_in_Minutes</p>
<p>Data outlier ini dihilangkan dengan menggunakan metode 1.5 interquantile range (IQR)</p>
"""

Q1 = data[numerical_features].quantile(0.25)
Q3 = data[numerical_features].quantile(0.75)
IQR=Q3-Q1
data=data[~((data[numerical_features]<(Q1-1.5*IQR))|(data[numerical_features]>(Q3+1.5*IQR))).any(axis=1)]
data

"""<p>setelah pembersihan data-data outlier menggunakan outlier, terdapat 72001 data. Variabel target kembali dihitung kembali untuk mengantisipasi data yang kurang seimbang</p>"""

count = data["satisfaction"].value_counts()
percent = 100*data["satisfaction"].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title="Satisfaction",rot=0);

"""<h4>Data kepuasan berimbang setelah 1.5xIQR</h4>

<h2>Data Preprocessing</h2>
<h3>Encoding Categorical Features</h3>
<p>Terdapat beberapa variabel kategorikal memberikan nilai keterangan. Hal ini tidak dapat dimodelkan. Oleh karena itu, variabel-variabel perlu dilakukan encoding agar dapat diproses oleh algoritma pemodelan.</p>
<p>Variabel Gender, Type_of_Travel, Class, dan Customer_Type dilakukan one-hot encoding karena nilai datanya berupa keterangan. Sedangkan, nilai-nilai data dalam variabel Satisfaction ditransformasi ke angka biner(0 dan 1) karena datanya hanya dua dan lebih mudah dilakukan untuk binary classification</p>
"""

from sklearn.preprocessing import  OneHotEncoder
from sklearn.preprocessing import LabelEncoder
data = pd.concat([data, pd.get_dummies(data['Gender'], prefix='Gender')],axis=1)
data = pd.concat([data, pd.get_dummies(data['Type_of_Travel'], prefix='Travel')],axis=1)
data = pd.concat([data, pd.get_dummies(data['Class'], prefix='Class')],axis=1)
data = pd.concat([data, pd.get_dummies(data['Customer_Type'], prefix='Customer_Type')],axis=1)

le = LabelEncoder()
data["satisfaction"]=le.fit_transform(data["satisfaction"])
data.drop(['Gender',"Type_of_Travel","Class","Customer_Type"],axis=1,inplace=True)

data

"""<h2>Test-train split</h2>

Data di bagi menjadi data training 80% dan data test 20%
"""

from sklearn.model_selection import train_test_split

X = data.drop("satisfaction",axis=1)
y = data['satisfaction']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""<h2>Standard Scaling training numerical features </h2>
<p>Data training dilakukan standard scaling untuk mempercepat proses pembelajaran. proses scaling data training dan data test dilakukan secara terpisah</p>
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features=["Age","Flight_Distance","Departure_Delay_in_Minutes","Arrival_Delay_in_Minutes"]
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].describe().round(2)

"""<h3>Model Development</h3>
Dilakukan 3 fitting model: KNN, Random Forest dan AdaBoosting
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_acc', 'test_acc'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error

knn = KNeighborsClassifier(n_neighbors=15)
knn.fit(X_train, y_train)

models.loc['train_acc','KNN'] = knn.score(X_train,y_train)
print(models)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100,max_depth=None,)
rf.fit(X_train,y_train)
models.loc['train_acc','RandomForest'] = rf.score(X_train,y_train)
print(models)

from sklearn.ensemble import AdaBoostClassifier

boosting = AdaBoostClassifier(learning_rate=0.1, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_acc','Boosting'] = boosting.score(X_train,y_train)
print(models)

"""Algoritma Random Forest memberikan score tertinggi diantara algoritma lainnya

<h2>Standard Scaling test numerical features </h2>
data test dilakukan standard scaling secara terpisah dengan data training
"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""<h3>Model Evaluation</h3>"""

#model evaluation
acc = pd.DataFrame()
from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score
model_dict = {'KNN': knn, "Ada Boosting":boosting,'RF': rf}
for name, model in model_dict.items():
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    # acc.loc[name, 'Accuracy_train'] = accuracy_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Precision_train"] = precision_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Recall_train"] = recall_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "F1_score_train"] = f1_score(y_true=y_train,y_pred=y_pred_train)
    acc.loc[name, 'Accuracy_test'] = accuracy_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Precision_test"] = precision_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Recall_test"] = recall_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "F1_score_test"] = f1_score(y_true=y_test,y_pred=y_pred_test)

acc

"""Algoritma Randomforest pada data test memberikan nilai peforma yang sangat baik

<h3>Hypertuning Parameters</h3>
<h4>Menggunakan GridSearchCV</h4>
Hypertuning parameter dilakukan untuk mencari parameter yang lebih baik dibandingkan parameter yang digunakan pada model
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50,100,200, 500],
    'max_features': ['sqrt', 'log2'],
    'max_depth' : [None],
    'criterion' :['gini', 'entropy']
}
CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5,verbose=5)
CV_rfc.fit(X_train, y_train)
CV_rfc.best_params_

"""<h3>Fitting data dengan parameter hasil GridSeachCV</h3>
Hasil parameter terbaik dari hyperparameter tuning dengan GridSeachCV diset ke parameter model dan diukur kembali performanya.
"""

print(CV_rfc.best_params_)
best_params=CV_rfc.best_params_
knn = RandomForestClassifier(criterion=best_params['criterion'],max_depth=best_params['max_depth'],
                            max_features=best_params['max_features'],n_estimators=best_params['n_estimators'])
knn.fit(X_train,y_train)
model_dict_ = {'RF_grid_search': knn}
acc1 = pd.DataFrame()


for name, model in model_dict_.items():
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    # acc.loc[name, 'Accuracy_train'] = accuracy_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Precision_train"] = precision_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Recall_train"] = recall_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "F1_score_train"] = f1_score(y_true=y_train,y_pred=y_pred_train)
    acc.loc[name, 'Accuracy_test'] = accuracy_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Precision_test"] = precision_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Recall_test"] = recall_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "F1_score_test"] = f1_score(y_true=y_test,y_pred=y_pred_test)
acc

"""parameter hasil GridSeachCV memberikan hasil yang sedikit lebih baik

<h2>Confusion Matrix</h2>
"""

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(rf,X_test,y_test,display_labels=['neutral or dissatisfied','satisfied'])
plt.show()

pip freeze > requirements.txt

