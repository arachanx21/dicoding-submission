# -*- coding: utf-8 -*-
"""Predictive_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sCv04Q_27RPMZKrEyrfm7P2lAd-F2bPQ

<h2>Data Loading</h2>
"""

import kagglehub, os
import pandas as pd
dir = kagglehub.dataset_download('deltasierra452/airline-pax-satisfaction-survey')

print('Data source import complete.')
data = pd.read_csv(os.path.join(dir,os.listdir(dir)[0]))
data

"""Berdasarkan data di atas, terdapat 103904 baris data dengan 25 kolom."""

#menggunakan environment kaggle
import pandas as pd
data=pd.read_csv("/kaggle/input/airline-pax-satisfaction-survey/airline_clnd.csv")
data

data.info()

"""Berdasarkan data di atas, variabel Gender, Customer_type, Type_of_Travel, Class, dan satisfaction bertipe **object**, sedangkan Arrival_Delay_in_Minutes bertipe **float64** dan sisanya bertipe **int64**."""

data.describe()

"""Berdasarkan data di atas, tidak terlihat bahwa terdapat missing value karena range angka-angka yang terdapat di data sesuai dengan spesifikasi data di dokumentasi data set. Namun, data dengan nilai kosong masih perlu diidentifikasi dengan mengecek apakah terdapat data kosong pada setiap kolom."""

print("number of missing data:")
for i in data.columns:
  print(i,"\t:",data[i].loc[data[i].isnull()].shape[0])

"""Berdasarkan data di atas, terdapat 310 data kosong di variabel Arrival_Delay_in_Minutes.

Data dengan nilai kosong dihilangkan dengan fungsi built-in pandas dropna.
"""

data = data.dropna()
data

"""Terdapat 310 data yang kosong. Setelah data kosong dihilangkan, dataset dicek kembali apakah terdapat data-data duplikat menggunakan fungsi built-in pandas drop_duplicates()

<h4>Memeriksa apakah dataset memiliki data duplikat</h4>
"""

data.duplicated().sum()

"""Berdasarkan hasil di atas, tidak ada data duplikat dalam data set."""

data = data.drop_duplicates()
data

"""Setelah pengecekan, tidak ada perubahan jumlah baris dalam data, sehingga data ini tidak memiliki duplikat.

Kolom SR dan ID adalah kolom yang berhubungan dengan identitas pelanggan. Hal ini tidak akan mengganggu proses pemodelan. Sehingga variabel-variabel ini dihapus
"""

data=data.drop("SR",axis=1)
data=data.drop("id",axis=1)
data

"""Sekarang dataset memiliki 103904 baris dan 23 variabel.

<h2>Data Exploration</h2>
<h5>Mengelompokkan Data berdasarkan group umur</h5>
"""

age_groups={
    "0-9":data.Age.loc[data.Age<10].shape[0],
    "10-19":data.Age.loc[(data.Age>10)&(data.Age<=20)].shape[0],
    "20-29":data.Age.loc[(data.Age>20)&(data.Age<=30)].shape[0],
    "30-39":data.Age.loc[(data.Age>30)&(data.Age<=40)].shape[0],
    "40-49":data.Age.loc[(data.Age>40)&(data.Age<=50)].shape[0],
    "50-59":data.Age.loc[(data.Age>50)&(data.Age<=60)].shape[0],
    "60-69":data.Age.loc[(data.Age>60)&(data.Age<=70)].shape[0],
    "70-79":data.Age.loc[(data.Age>70)&(data.Age<=80)].shape[0],
    "80-89":data.Age.loc[(data.Age>80)&(data.Age<=90)].shape[0],
}
print(age_groups)

import numpy as np
x = np.array(list(age_groups.values()))
print("Age groups:")
for i in age_groups.keys():
    print(i,"\tPercentage:",age_groups[i]/x.sum()*100,"%")

"""Lebih dari 80% Penumpang berumur 20-60. Kelompok umur difokuskan kepada penumpang berumumur diantara 20 dan 60 tahun"""

#mengidentifikasi penumpang tidak loyal berumumur 20-60
# data=data.loc[data["Customer_Type"]=="disloyal Customer"]
data=data.loc[(data.Age>=20)&(data.Age<=60)]
# data=data.drop("Customer_Type",axis=1)
data=data.dropna()

"""<h3>Mengeksplorasi data target(satisfaction)</h3>"""

count = data["satisfaction"].value_counts()
percent = 100*data["satisfaction"].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title="Satisfaction",rot=0)

"""<h4>Data kepuasan pelanggan menunjukkan bahwa data relatif berimbang. Pengurangan data tidak diperlukan untuk mengurangi bias pada pemodelan</h4>

<h2>Univariate Data analysis</h2>
"""

data.describe().round(2)

"""Setelah penghilangan data dengan nilai kosong dan seleksi kelompok umur dari data set, terdapat 85865 baris data total."""

import matplotlib.pyplot as plt
data.hist(bins=50,figsize=(20,20))
plt.show()

"""Berdasarkan informasi dari dataset dan data yang terbaca di pandas, beberapa variabel numerik meliputi: Age, Flight_Distance, Departure_Delay_in_Minutes, dan Arrival_Delay_in_Minutes"""

numerical_features=["Age","Flight_Distance","Departure_Delay_in_Minutes","Arrival_Delay_in_Minutes"]
features=list(data.columns)
for i in numerical_features:
    features.remove(i)
data[features]

"""Variabel Gender, Customer_Type, Type_of_Travel, Class, dan satisfaction merupakan variabel numerik yang memberikan nilai data non-numerik. Sehingga variabel-variabel tersebut harus di encode terlebih dahulu(Tahapan Encoding di bawah)

<h2>Exploratory Data - Numerical Features</h2>
"""

data[numerical_features].hist()
plt.show()

"""Variabel Age relatif rata, sedangkan variabel numerik lainnya memiliki kecenderungan left-skewed.

<h2>Mengidentifikasi outlier pada variabel-variabel numerik</h2>
outlier diidentifikasi menggunakan boxplot
"""

import seaborn as sns
sns.boxplot(data[numerical_features])
plt.xticks(rotation=90)
plt.show()

"""<p>Berdasarkan grafik di atas, terdapat 3 variabel numerik yang memiliki outlier. Variabel tersebut meliputi: Flight_Distance, Departure_Delay_in_Minutes dan Arrival_Delay_in_Minutes</p>
<p>Data outlier ini dihilangkan dengan menggunakan metode 1.5 interquantile range (IQR)</p>
"""

Q1 = data[numerical_features].quantile(0.25)
Q3 = data[numerical_features].quantile(0.75)
IQR=Q3-Q1
data=data[~((data[numerical_features]<(Q1-1.5*IQR))|(data[numerical_features]>(Q3+1.5*IQR))).any(axis=1)]
data

"""<p>setelah pembersihan data-data outlier menggunakan outlier, terdapat 72001 data. Variabel target kembali dihitung kembali untuk mengantisipasi data yang kurang seimbang</p>"""

count = data["satisfaction"].value_counts()
percent = 100*data["satisfaction"].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title="Satisfaction",rot=0);

"""<h4>Data kepuasan berimbang setelah 1.5xIQR</h4>

<h2>Data Preprocessing</h2>
<h3>Encoding Categorical Features</h3>
<p>Terdapat beberapa variabel kategorikal memberikan nilai keterangan. Hal ini tidak dapat dimodelkan. Oleh karena itu, variabel-variabel perlu dilakukan encoding agar dapat diproses oleh algoritma pemodelan.</p>
<p>Variabel Gender, Type_of_Travel, Class, dan Customer_Type dilakukan one-hot encoding karena nilai datanya berupa keterangan. Sedangkan, nilai-nilai data dalam variabel Satisfaction ditransformasi ke angka biner(0 dan 1) karena datanya hanya dua dan lebih mudah dilakukan untuk binary classification</p>
"""

from sklearn.preprocessing import  OneHotEncoder
from sklearn.preprocessing import LabelEncoder
data = pd.concat([data, pd.get_dummies(data['Gender'], prefix='Gender')],axis=1)
data = pd.concat([data, pd.get_dummies(data['Type_of_Travel'], prefix='Travel')],axis=1)
data = pd.concat([data, pd.get_dummies(data['Class'], prefix='Class')],axis=1)
data = pd.concat([data, pd.get_dummies(data['Customer_Type'], prefix='Customer_Type')],axis=1)

le = LabelEncoder()
data["satisfaction"]=le.fit_transform(data["satisfaction"])
data.drop(['Gender',"Type_of_Travel","Class","Customer_Type"],axis=1,inplace=True)

data

"""<h2>Test-train split</h2>

Data di bagi menjadi data training 80% dan data test 20%
"""

from sklearn.model_selection import train_test_split

X = data.drop("satisfaction",axis=1)
y = data['satisfaction']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""<h2>Standard Scaling training numerical features </h2>
<p>Data training dilakukan standard scaling untuk mempercepat proses pembelajaran. proses scaling data training dan data test dilakukan secara terpisah</p>
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features=["Age","Flight_Distance","Departure_Delay_in_Minutes","Arrival_Delay_in_Minutes"]
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].describe().round(2)

"""<h3>Model Development</h3>
<p>Dilakukan 3 fitting model: KNN, Random Forest dan AdaBoosting. Karena pemodelan ini merupakan pemodelan yang memprediksi apakah pelanggan puas dengan layanan maskapai, pemodelan klasifikasi dipilih sebagai model prekdisi.<p>
<p>Model <b>klasifikasi</b> ini memprediksi suatu nilai diskrit (puas atau tidak puas) berdarkan vektor pada fitur-fitur.</p>
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_acc', 'test_acc'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""<h3>K-Nearest Neighbor(KNN)</h3>
<p>Algoritma KNN membandingkan jarak satu sampel data ke sampel data pelatihan lain dengan memilih sejumlah k tetangga terdekat untuk memprediksi variabel objek. </p>
<p>Parameter utama dalam algoritma ini adalah n_neighbors, yaitu jumlah tetangga yang akan dibandingkan untuk mengklasifikasikan hasil prediksi.</p>
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error

knn = KNeighborsClassifier(n_neighbors=15)
knn.fit(X_train, y_train)

models.loc['train_acc','KNN'] = knn.score(X_train,y_train)
print(models)

"""akurasi training menggunakan algoritma KNN memberikan hasil yang sangat baik (93.3%)

<h3>Random forest</h3>
Algoritma Random forest menggunakan beberapa model <b>decision tree</b> (pengambilan keputusan berdasarkan kondisional) yang independen dan digabung menjadi satu kesatuan (bagging). <b>Algoritma decision tree</b> memprediksi suatu hasil berdasarkan kondisi-kondisi fitur. Model dengan algoritma ini memprediksi berdasarkan pendekatan if-else dari kondisi fitur dalam data training dalam membuat aturan dalam model. Algoritma decision tree ini secara indpenden digabungkan <b>(bagging)</b> menghasilkan suatu prediksi. Dalam algortma ini, parameter n_estimator menunjukkan jumlah model decision tree yang akan digabungkan. max_depth menunjukkan kedalaman dari decision tree, nilai None menunjukkan bahwa kedalaman decision tree dalam model tersebut tidak terbatas.
"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100,max_depth=None,)
rf.fit(X_train,y_train)
models.loc['train_acc','RandomForest'] = rf.score(X_train,y_train)
print(models)

"""<h3>Ada Boosting</h3>
Algoritma Boosting memiliki pendekatan ensemble yang mirip dengan algoritma random forest. Metode ini memanfaatkan penggabungan model dengan model terbaru memperbaiki model sebelumnya hingga menciptakan model yang lebih baik. Pada algoritma Ada Boost, Awalnya, semua kasus dalam data latih memiliki weight atau bobot yang sama. Setiap tahap, bobot akan dirubah bergantung ketepatannya, tahapan ini terus dilakukan hingga model mencapai akurasi yang diinginkan.

Secara default, AdaBoostClassifier menggunakan algoritma decision tree dan memiliki model maksimum sebanyak 50. Parameter utama dalam algoritma ini adalah learning_rate, atau laju pembelajaran di mana beban yang akan diberikan pada setiap iterasi boosting.
"""

from sklearn.ensemble import AdaBoostClassifier

boosting = AdaBoostClassifier(learning_rate=0.1, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_acc','Boosting'] = boosting.score(X_train,y_train)
print(models)

"""Algoritma Random Forest memberikan score tertinggi diantara algoritma lainnya

<h2>Standard Scaling test numerical features </h2>
data test dilakukan standard scaling secara terpisah dengan data training
"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""<h3>Model Evaluation</h3>"""

#model evaluation
acc = pd.DataFrame()
from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score
model_dict = {'KNN': knn, "Ada Boosting":boosting,'RF': rf}
for name, model in model_dict.items():
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    # acc.loc[name, 'Accuracy_train'] = accuracy_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Precision_train"] = precision_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Recall_train"] = recall_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "F1_score_train"] = f1_score(y_true=y_train,y_pred=y_pred_train)
    acc.loc[name, 'Accuracy_test'] = accuracy_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Precision_test"] = precision_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Recall_test"] = recall_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "F1_score_test"] = f1_score(y_true=y_test,y_pred=y_pred_test)

acc

"""Algoritma Randomforest pada data test memberikan nilai peforma yang sangat baik

<h3>Hypertuning Parameters</h3>
<h4>Menggunakan GridSearchCV</h4>
Hypertuning parameter dilakukan untuk mencari parameter yang lebih baik dibandingkan parameter yang digunakan pada model.

<p>n_estimators: jumlah decision tree pada model</p>
<p>max_depth: kedalaman decision tree (None: tidak terbatas)</p>
<p>max_features: jumlah fitur yang dipertimbangkan untuk mencari data_split terbaik. sqrt=sebanyak akar jumlah fitur, log2=sebanyak logaritma fitur dengan basis 2</p>
<p>criterion:fungsi untuk mengukur kualitas split. gini=menggunakan impunitas gini, entropy=menggunakan shannon information gain.</p>
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50,100,200, 500],
    'max_features': ['sqrt', 'log2'],
    'max_depth' : [None],
    'criterion' :['gini', 'entropy']
}
CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5,verbose=5)
CV_rfc.fit(X_train, y_train)
CV_rfc.best_params_

"""<h3>Fitting data dengan parameter hasil GridSeachCV</h3>
Hasil parameter terbaik dari hyperparameter tuning dengan GridSeachCV diset ke parameter model dan diukur kembali performanya.
"""

print(CV_rfc.best_params_)
best_params=CV_rfc.best_params_
knn = RandomForestClassifier(criterion=best_params['criterion'],max_depth=best_params['max_depth'],
                            max_features=best_params['max_features'],n_estimators=best_params['n_estimators'])
knn.fit(X_train,y_train)
model_dict_ = {'RF_grid_search': knn}
acc1 = pd.DataFrame()


for name, model in model_dict_.items():
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    # acc.loc[name, 'Accuracy_train'] = accuracy_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Precision_train"] = precision_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "Recall_train"] = recall_score(y_true=y_train,y_pred=y_pred_train)
    # acc.loc[name, "F1_score_train"] = f1_score(y_true=y_train,y_pred=y_pred_train)
    acc.loc[name, 'Accuracy_test'] = accuracy_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Precision_test"] = precision_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "Recall_test"] = recall_score(y_true=y_test,y_pred=y_pred_test)
    acc.loc[name, "F1_score_test"] = f1_score(y_true=y_test,y_pred=y_pred_test)
acc

"""parameter hasil GridSeachCV memberikan hasil yang sedikit lebih baik

<h2>Confusion Matrix</h2>
"""

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_estimator(rf,X_test,y_test,display_labels=['neutral or dissatisfied','satisfied'])
plt.show()

"""berdasarkan confusion matrix, model memiliki akurasi 97% mengidentifikasi penumpang yang netral dan tidak puas dan 95% mengidentifikasi penumpang yang puas. Dengan ini model dapat digunakan untuk mempelajari aspek mana yang perlu ditingkatkan oleh maskapai dengan memberikan data-data dengan fitur-fitur terkontrol kepada model."""

pip freeze > requirements.txt

